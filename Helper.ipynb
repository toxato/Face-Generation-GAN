{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import hashlib\n",
    "#from urllib.request import urlretrieve\n",
    "from urllib import urlretrieve\n",
    "import zipfile\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _read32(bytestream):\n",
    "    \"\"\"\n",
    "    Read 32-bit integer from bytesteam\n",
    "    :param bytestream: A bytestream\n",
    "    :return: 32-bit integer\n",
    "    \"\"\"\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "\n",
    "def _ungzip(save_path, extract_path, database_name, _):\n",
    "    \"\"\"\n",
    "    Unzip a gzip file and extract it to extract_path\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param extract_path: The location to extract the data to\n",
    "    :param database_name: Name of database\n",
    "    :param _: HACK - Used to have to same interface as _unzip\n",
    "    \"\"\"\n",
    "    # Get data from save_path\n",
    "    with open(save_path, 'rb') as f:\n",
    "        with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "            magic = _read32(bytestream)\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Invalid magic number {} in file: {}'.format(magic, f.name))\n",
    "            num_images = _read32(bytestream)\n",
    "            rows = _read32(bytestream)\n",
    "            cols = _read32(bytestream)\n",
    "            buf = bytestream.read(rows * cols * num_images)\n",
    "            data = np.frombuffer(buf, dtype=np.uint8)\n",
    "            data = data.reshape(num_images, rows, cols)\n",
    "\n",
    "    # Save data to extract_path\n",
    "    for image_i, image in enumerate(\n",
    "            tqdm(data, unit='File', unit_scale=True, miniters=1, desc='Extracting {}'.format(database_name))):\n",
    "        Image.fromarray(image, 'L').save(os.path.join(extract_path, 'image_{}.jpg'.format(image_i)))\n",
    "\n",
    "\n",
    "def get_image(image_path, width, height, mode):\n",
    "    \"\"\"\n",
    "    Read image from image_path\n",
    "    :param image_path: Path of image\n",
    "    :param width: Width of image\n",
    "    :param height: Height of image\n",
    "    :param mode: Mode of image\n",
    "    :return: Image data\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    if image.size != (width, height):  # HACK - Check if image is from the CELEBA dataset\n",
    "        # Remove most pixels that aren't part of a face\n",
    "        face_width = face_height = 108\n",
    "        j = (image.size[0] - face_width) // 2\n",
    "        i = (image.size[1] - face_height) // 2\n",
    "        image = image.crop([j, i, j + face_width, i + face_height])\n",
    "        image = image.resize([width, height], Image.BILINEAR)\n",
    "\n",
    "    return np.array(image.convert(mode))\n",
    "\n",
    "\n",
    "def get_batch(image_files, width, height, mode):\n",
    "    data_batch = np.array(\n",
    "        [get_image(sample_file, width, height, mode) for sample_file in image_files]).astype(np.float32)\n",
    "\n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "\n",
    "    return data_batch\n",
    "\n",
    "\n",
    "def images_square_grid(images, mode):\n",
    "    \"\"\"\n",
    "    Save images as a square grid\n",
    "    :param images: Images to be used for the grid\n",
    "    :param mode: The mode to use for images\n",
    "    :return: Image of images in a square grid\n",
    "    \"\"\"\n",
    "    # Get maximum size for square grid of images\n",
    "    save_size = math.floor(np.sqrt(images.shape[0]))\n",
    "\n",
    "    # Scale to 0-255\n",
    "    images = (((images - images.min()) * 255) / (images.max() - images.min())).astype(np.uint8)\n",
    "\n",
    "    # Put images in a square arrangement\n",
    "    images_in_square = np.reshape(\n",
    "            images[:save_size*save_size],\n",
    "            (save_size, save_size, images.shape[1], images.shape[2], images.shape[3]))\n",
    "    if mode == 'L':\n",
    "        images_in_square = np.squeeze(images_in_square, 4)\n",
    "\n",
    "    # Combine images to grid image\n",
    "    new_im = Image.new(mode, (images.shape[1] * save_size, images.shape[2] * save_size))\n",
    "    for col_i, col_images in enumerate(images_in_square):\n",
    "        for image_i, image in enumerate(col_images):\n",
    "            im = Image.fromarray(image, mode)\n",
    "            new_im.paste(im, (col_i * images.shape[1], image_i * images.shape[2]))\n",
    "\n",
    "    return new_im\n",
    "\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_CELEBA_NAME = 'celeba'\n",
    "    DATASET_MNIST_NAME = 'mnist'\n",
    "\n",
    "    if database_name == DATASET_CELEBA_NAME:\n",
    "        url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip'\n",
    "        hash_code = '00d2c5bc6d35e252742224ab0c1e8fcb'\n",
    "        extract_path = os.path.join(data_path, 'img_align_celeba')\n",
    "        save_path = os.path.join(data_path, 'celeba.zip')\n",
    "        extract_fn = _unzip\n",
    "    elif database_name == DATASET_MNIST_NAME:\n",
    "        url = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "        hash_code = 'f68b3c2dcbeaaa9fbdd348bbdeb94873'\n",
    "        extract_path = os.path.join(data_path, 'mnist')\n",
    "        save_path = os.path.join(data_path, 'train-images-idx3-ubyte.gz')\n",
    "        extract_fn = _ungzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    # Remove compressed data\n",
    "    os.remove(save_path)\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name, data_files):\n",
    "        \"\"\"\n",
    "        Initalize the class\n",
    "        :param dataset_name: Database name\n",
    "        :param data_files: List of files in the database\n",
    "        \"\"\"\n",
    "        DATASET_CELEBA_NAME = 'celeba'\n",
    "        DATASET_MNIST_NAME = 'mnist'\n",
    "        IMAGE_WIDTH = 28\n",
    "        IMAGE_HEIGHT = 28\n",
    "\n",
    "        if dataset_name == DATASET_CELEBA_NAME:\n",
    "            self.image_mode = 'RGB'\n",
    "            image_channels = 3\n",
    "\n",
    "        elif dataset_name == DATASET_MNIST_NAME:\n",
    "            self.image_mode = 'L'\n",
    "            image_channels = 1\n",
    "\n",
    "        self.data_files = data_files\n",
    "        self.shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, image_channels\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate batches\n",
    "        :param batch_size: Batch Size\n",
    "        :return: Batches of data\n",
    "        \"\"\"\n",
    "        IMAGE_MAX_VALUE = 255\n",
    "\n",
    "        current_index = 0\n",
    "        while current_index + batch_size <= self.shape[0]:\n",
    "            data_batch = get_batch(\n",
    "                self.data_files[current_index:current_index + batch_size],\n",
    "                self.shape[1],\n",
    "                self.shape[2],\n",
    "                self.shape[3], # *self.shape[1:3],\n",
    "                self.image_mode)\n",
    "\n",
    "            current_index += batch_size\n",
    "\n",
    "            yield data_batch / IMAGE_MAX_VALUE - 0.5\n",
    "\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
